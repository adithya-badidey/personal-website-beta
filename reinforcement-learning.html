
<!doctype html>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

  <meta name="generator" content="TiddlyWiki" />
  <meta name="tiddlywiki-version" content="5.1.23" />

  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
  <meta name="mobile-web-app-capable" content="yes" />

  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <meta name="format-detection" content="telephone=no">
	<script></script>
  <link id="faviconLink" rel="shortcut icon" href="/favicon.ico">

<title>Reinforcement Learning ~ Adithya's Lair</title>
<meta name="description" content="This is a review of my notes on Reinforcement Learning from CS6300 Artificial Intelligence.">

<meta property="og:type" content="article" />
<meta property="og:url" content="https://adithyab.in/reinforcement-learning" />
<meta property="og:title" content="Reinforcement Learning ~ Adithya's Lair" />
<meta property="og:image" content="" />
<meta property="og:description" content="This is a review of my notes on Reinforcement Learning from CS6300 Artificial Intelligence." />

<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@adithya_badidey">
<meta name="twitter:creator" content="@adithya_badidey">
<meta name="twitter:title" content="Reinforcement Learning ~ Adithya's Lair" />
<meta name="twitter:description" content="This is a review of my notes on Reinforcement Learning from CS6300 Artificial Intelligence." />
<meta name="twitter:image" content="" />

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-171642977-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }
  gtag('js', new Date());

  gtag('config', 'UA-171642977-1');

  document.addEventListener("DOMContentLoaded", () => {
    const e = Array.prototype.slice.call(document.querySelectorAll(".navbar-burger"), 0);
    e.length > 0 && e.forEach(e => {
      e.addEventListener("click", () => {
        const t = e.dataset.target,
          a = document.getElementById(t);
        e.classList.toggle("is-active"), a.classList.toggle("is-active")
      })
    })
  });
</script>

<link rel="stylesheet" href="css/style.css" />
</head>

<body class="tc-body">
  
<nav aria-label="main navigation" class="navbar" role="navigation">
    <div class="navbar-brand">
      <a class="navbar-item" href="/">
        Adithya's Lair
      </a>

      <a aria-expanded="false" aria-label="menu" class="navbar-burger burger" data-target="navbarTW" role="button">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>

    <div class="navbar-menu" id="navbarTW">
      <div class="navbar-end">
<a class="navbar-item" href="/">Home</a>
<a class="navbar-item" href="/blog">Blog</a>
<a class="navbar-item" href="/portfolio">Portfolio</a>
     </div>
    </div>
  </nav>


<section class="section">
    <div class="columns mt-6">
        <div class="column is-5-fullhd is-6-desktop is-8-tablet is-offset-2">
            <div class="content tc-blog">
                
                    <article class="message is-small">
                        <div class="message-body">
                            This note is a part of my Zettelkasten. What is below might not be complete or accurate. It
                            is also likely to change often.
                        </div>
                    </article>
                
                <span class="has-text-grey-light is-small tc-subtitle tc-created">
                    20th August, 2021
                </span>
                <h1 class="tc-title mt-1">
                    Reinforcement Learning
                </h1>
                <p>This is a review of my notes on Reinforcement Learning from <a class="tc-tiddlylink tc-tiddlylink-resolves" href="./artificial-intelligence-course-notes.html">CS6300 Artificial Intelligence</a>. </p><p>The goal of Reinforcement Learning is to use observations to learn an optimal policy in a static, Markovian Environment where the agent doesnt know the environment or the results of its actions. The agent faces a definite but unknown <a class="tc-tiddlylink tc-tiddlylink-resolves" href="./markov-decision-processes.html">MDP</a>.</p><p>Reinforcement Learning can be one of two types: Model based or model free. In model based, we try to estimate the transition probabilities and rewards function like in a regular MDP. In model free, we try to calculate the Utilities and Policy of the states directly.</p><p>There are two modes of reinforcement learning:</p><h2 class="">Passive Reinforcement Learning</h2><p>The goal of the passive reinforcement learning is to understand the model using observations of an agent, which follows a fixed policy throughout the learning process. </p><h3 class="">Model based learning</h3><p>We learn the model through the observations.</p><ul><li>For each state, calculate<ul><li>Transition probabilities: T(s,a,s') = Count(s' on (s,a))/count(s,a)</li><li>Reward function R(s,a,s') = First observed (R(s,a,s'))</li></ul></li></ul><h3 class="">Direct Evaluation</h3><p>This is a type of model-free learning where we calculate the utility of a state using the sum of rewards upto to the end state, at the end of all observations.</p><blockquote><div>v(s) = (Sum of rewards at s)/(number of times visited s)</div></blockquote><p>This will give the right solution but is very wasteful, since it ignores all other datapoints except the rewards</p><h3 class="">TD Learning</h3><p>TD Learning is a model-free learning technique where we learn V(s) from every observed transition (s, a, s, r). The key insight is that the implicit transition probabilities will weight the contribution from each s' propotional to how likely that outcome is.</p><p>For given policy pi, repeat for every (s,a,s',r): </p><blockquote><div>V(s) = (1-alpha) . V(s) + alpha (r + gamma.V(s'))</div></blockquote><p>where alpha is the learning rate (which can be a function of time in order to converge on a solution) and gamma is the discounting factor standand in bellman's equations.</p><h2 class="">Active Reinforced Learning</h2><p>The goal of active reinforcement learning is to be in active control of an agent while learning and solving the policy.</p><h3 class="">Exploration vs Exploitation</h3><p>An agent must choose between exploration and exploitation. There is a tradeoff.</p><h3 class="">Random exploration</h3><p>One way to solve the exploration vs exploration dilemma is to force exploration through random actions till we have an optimal policy</p><ul><li>At every state s, take a random number b = random[0,1]. <ul><li>If b &gt; epsilon, act as per current policy. </li><li>Else take any action among A(s) with equal probability. </li></ul></li><li>Increase epsilon with time.</li></ul><p>One problem with this is that randomly choosing is not that great. We should choose to get most amount of information. That can be done with the exploration function.</p><h3 class="">Exploration function</h3><p>The exploration function should prioritize any states which havn't been visited much. It should converge with the actual utility function as time passes.</p><blockquote><div>Exploration function f(u,n) = u + k/n</div></blockquote><p>where u is best utility estimate, k is a constant and n is number of times a state has been visited</p><h2 class="">Q-Learning with exploration</h2><p>Q-Learning is similar to TD-learning but using Q(s,a) instead of using V(S). </p><blockquote><div>Q(s,a) = (1 - alpha).Q(s,a) + alpha[R(s,a,s') + gamma.max<sub>a'</sub>Q(s',a')]</div></blockquote><p>adding on the exploration function</p><blockquote><div>Q(s,a) = (1 - alpha).Q(s,a) + alpha[R(s,a,s') + gamma.max<sub>a'</sub>f(Q(s',a'),n)]</div></blockquote><h2 class="">SARSA</h2><p>Very similar to Q-learning except that it uses the actual observations to calculate the Q-value instead of using max<sub>a'</sub>.</p><blockquote><div>Q(s,a) = (1 - alpha).Q(s,a) + alpha[R(s,a,s') + gamma.Q(s',a')]</div></blockquote><h2 class="">Functional Approximation</h2><p>In most real world situations, it is impossible to handle calculations of all possible states. It might be easier to look at modelling the world using continuous features.</p><h3 class="">Linear Regression</h3><blockquote><div>V(a) = w<sub>0</sub> + w<sub>1</sub>.f<sub>1</sub>(s) + w<sub>2</sub>.f<sub>2</sub>(s) ...</div><div>Q(s,a) = w<sub>0</sub> + w<sub>1</sub>.f<sub>1</sub>(s) + w<sub>2</sub>.f<sub>2</sub>(s) ...</div></blockquote><p>If features are chosen appropriately, experience can be summed up very succinctly. This allows good generalization from learned to unlearned states.</p><h3 class="">Approximate Q-Learning</h3><p>Use functional approximation to estimate the Q(s,a) values in order to learn. This scales much better if we get good features.
</p>
            </div>
            <div class="tc-blog-footer">
                <hr class="mb-2">
                <nav class="tc-backlinks mb-4">
                    <h2 class="backlinks-header">
                        
                            This page has been linked from:
                        
                    </h2>
                    <div class="content">
                        <ul>
                            
                                <li>
                                    <a class="tc-tiddlylink tc-tiddlylink-resolves" href="./markov-decision-processes.html">
                                        Markov Decision Processes
                                    </a>
                                </li>
                            
                        </ul>
                    </div>
                </nav>
                <nav class="level is-mobile">
                    
                    <div class="level-left">
                        <p class="is-size-7 has-text-grey-light is-small tc-modified tc-subtitle">Last Updated: 
                            8th September, 2021
                        </p>
                    </div>
                    <div class="level-right">
                        <div class="tags">
                            
                        </div>
                    </div>
                </nav>
                <hr>
            </div>
            <div id="disqus_thread"></div>
            <noscript>Please enable JavaScript to view the comments.</noscript>
        </div>
    </div>
</section>

<footer class="footer">
  <div class="columns">
	  <div class="column is-6-desktop is-8-tablet is-offset-2"><p><p>
Built with <a class="tc-tiddlylink-external" href="https://tiddlyjam.com" rel="noopener noreferrer" target="_blank">TiddlyJam</a> and <a class="tc-tiddlylink-external" href="https://bulma.io/" rel="noopener noreferrer" target="_blank">Bulma</a>. Hosted on <a class="tc-tiddlylink-external" href="https://pages.github.com/" rel="noopener noreferrer" target="_blank">Github Pages</a>.
    </p>
		</p></div>
  </div>
	<link href="https://fonts.googleapis.com/css2?family=Inconsolata:wght@400;600&amp;display=swap" rel="stylesheet">
</footer>

</body>
</html>
